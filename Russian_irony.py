# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gkD0j5jGdlKOkU-s3rFKBtsVxIPvQILu

# 1. Импорт библиотек и загрузка ресурсов
"""

!pip install pandas numpy matplotlib seaborn nltk spacy scikit-learn umap-learn openpyxl
!python -m spacy download ru_core_news_sm

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score
from sklearn.cluster import KMeans
import umap.umap_ as umap


nltk.download('punkt_tab')
nltk.download('stopwords')
nlp = spacy.load('ru_core_news_sm')

"""2. Функции очистки и предобработки текста"""

stop_words = set(stopwords.words("russian")) | {
    "это", "который", "еще", "так", "вот", "такой", "тот", "она", "он", "они", "мы", "вы",
    "а", "но", "да", "если", "или", "к", "с", "у", "по", "из", "за", "от", "для", "то",
    "быть", "бы", "не", "ну", "же", "очень", "свой", "тебе", "пока", "наш", "ради", "каждый", "которая", "которую", "которые", "сколько", "угодно", "слишком", "всё", "неё", "чём"
}
stop_words = list(stop_words)

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^а-яё\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def preprocess_text(text):
    text = clean_text(text)
    tokens = word_tokenize(text, language="russian")
    tokens = [token for token in tokens if token not in stop_words]
    doc = nlp(' '.join(tokens))
    lemmas = [token.lemma_ for token in doc if token.lemma_ not in stop_words]
    return ' '.join(lemmas)

"""3. Загрузка данных и предобработка"""

df = pd.read_excel('irony_dataset.xlsx')
df['processed_text'] = df['Content'].apply(preprocess_text)

def get_top_n_words(corpus, n=20, stop_words=None):
    vec = CountVectorizer(stop_words=stop_words).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]

top_words_irony = get_top_n_words(df[df['Class'] == 1]['processed_text'], n=20, stop_words=stop_words)
top_words_non_irony = get_top_n_words(df[df['Class'] == 0]['processed_text'], n=20, stop_words=stop_words)

def plot_top_words(words_freq, title):
    words, freqs = zip(*words_freq)
    plt.figure(figsize=(10, 5))
    sns.barplot(x=list(freqs), y=list(words), palette='coolwarm')
    plt.title(title)
    plt.xlabel("Частота")
    plt.ylabel("Слова")
    plt.tight_layout()
    plt.show()

plot_top_words(top_words_irony, "Частотные слова в ироничных текстах")
plot_top_words(top_words_non_irony, "Частотные слова в неироничных текстах")

"""4. TF-IDF векторизация"""

vectorizer = TfidfVectorizer(max_features=1000, stop_words=stop_words)
X_tfidf = vectorizer.fit_transform(df['processed_text'])

"""5. Разделение выборки и обучение модели"""

X_train, X_test, y_train, y_test = train_test_split(X_tfidf, df['Class'], test_size=0.2, random_state=42)
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

"""6. Оценка качества модели"""

y_pred = model.predict(X_test)
print("Отчет по классификации:")
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")

"""7. Кластеризация текстов KMean"""

num_clusters = 5
kmeans = KMeans(n_clusters=num_clusters, random_state=42)
df['cluster'] = kmeans.fit_predict(X_tfidf)
cluster_labels = kmeans.fit_predict(X_tfidf)

"""8. Определение топ-слов для каждого кластера"""

def get_top_n_words(corpus, n=20, stop_words=None):
    vec = CountVectorizer(stop_words=stop_words).fit(corpus)  # Передаём стоп-слова
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]

stop_words_list = list(stop_words)

top_words_irony = get_top_n_words(df[df['Class'] == 1]['processed_text'], n=20, stop_words=stop_words_list)
top_words_non_irony = get_top_n_words(df[df['Class'] == 0]['processed_text'], n=20, stop_words=stop_words_list)

for i in range(num_clusters):
    cluster_texts = df[df['cluster'] == i]['processed_text']
    top_words = get_top_n_words(cluster_texts)
    print(f"\nТоп слов для кластера {i}:")
    for word, freq in top_words:
        print(f"{word}: {freq}")

"""9. Визуализация кластеров с UMAP"""

X_dense = X_tfidf.toarray()
reducer = umap.UMAP(random_state=42)
embedding = reducer.fit_transform(X_dense)

plt.figure(figsize=(12, 7))
sns.scatterplot(x=embedding[:,0], y=embedding[:,1], hue=df['cluster'], palette='tab10', alpha=0.7)
plt.title('UMAP проекция с кластерами')
plt.xlabel('UMAP 1')
plt.ylabel('UMAP 2')
plt.legend(title='Кластер')
plt.show()

"""Определение тематики кластеров"""

import numpy as np

def get_cluster_top_words(X_tfidf, labels, vectorizer, top_n=10):
    feature_names = np.array(vectorizer.get_feature_names_out())
    cluster_top_words = {}
    for cluster_id in np.unique(labels):
        cluster_indices = np.where(labels == cluster_id)[0]
        cluster_tfidf_sum = np.array(X_tfidf[cluster_indices].sum(axis=0)).flatten()
        top_word_indices = cluster_tfidf_sum.argsort()[::-1][:top_n]
        top_words = [(feature_names[i], cluster_tfidf_sum[i]) for i in top_word_indices]
        cluster_top_words[cluster_id] = top_words
    return cluster_top_words

cluster_top_words = get_cluster_top_words(X_tfidf, cluster_labels, vectorizer, top_n=10)

print("\nТематические слова по кластерам:")
for cid, words in cluster_top_words.items():
    print(f"\nКластер {cid}:")
    for w, score in words:
        print(f"  {w} ({score:.2f})")

"""10. Функция для предсказания новых текстов"""

def predict_irony(text, model, vectorizer, nlp, stop_words):
    def clean_text(text):
        text = text.lower()
        text = re.sub(r'[^а-яё\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    def preprocess_text(text):
        text = clean_text(text)
        tokens = nltk.word_tokenize(text, language="russian")
        tokens = [token for token in tokens if token not in stop_words]
        doc = nlp(' '.join(tokens))
        lemmas = [token.lemma_ for token in doc if token.lemma_ not in stop_words]
        return ' '.join(lemmas)

    processed = preprocess_text(text)
    X_vec = vectorizer.transform([processed])
    pred = model.predict(X_vec)[0]
    proba = model.predict_proba(X_vec)[0][pred] if hasattr(model, "predict_proba") else None

    return pred, proba

text_new = "люблю дождь!"
prediction, probability = predict_irony(text_new, model, vectorizer, nlp, stop_words)
print(f"\nТекст: {text_new}")
print(f"Предсказание: {'Ирония' if prediction == 1 else 'Не ирония'}")
if probability:
    print(f"Уверенность модели: {probability:.2f}")