# -*- coding: utf-8 -*-
"""project_functions.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yf87SpvxF8T8S5hWd0_5ZHBjh6wWc_8-
"""

# project_functions.py

import re
import nltk
import spacy
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize

from sklearn.feature_extraction.text import CountVectorizer
import numpy as np

# Запуск
nlp = spacy.load("ru_core_news_sm")
nltk.download("punkt")
nltk.download("stopwords")

stop_words = set(stopwords.words("russian")) | {
    "это", "который", "еще", "так", "вот", "такой", "тот", "она", "он", "они", "мы", "вы",
    "а", "но", "да", "если", "или", "к", "с", "у", "по", "из", "за", "от", "для", "то",
    "быть", "бы", "не", "ну", "же", "очень", "свой", "тебе", "пока", "наш", "ради",
    "каждый", "которая", "которую", "которые", "сколько", "угодно", "слишком", "всё",
    "неё", "чём"
}
stop_words = list(stop_words)

def clean_text(text):
    text = text.lower()
    text = re.sub(r'[^а-яё\s]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def preprocess_text(text):
    text = clean_text(text)
    tokens = word_tokenize(text, language="russian")
    tokens = [token for token in tokens if token not in stop_words]
    doc = nlp(' '.join(tokens))
    lemmas = [token.lemma_ for token in doc if token.lemma_ not in stop_words]
    return ' '.join(lemmas)

def get_top_n_words(corpus, n=20, stop_words=None):
    vec = CountVectorizer(stop_words=stop_words).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0)
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    return sorted(words_freq, key=lambda x: x[1], reverse=True)[:n]

def get_cluster_top_words(X_tfidf, labels, vectorizer, top_n=10):
    feature_names = np.array(vectorizer.get_feature_names_out())
    cluster_top_words = {}
    for cluster_id in np.unique(labels):
        cluster_indices = np.where(labels == cluster_id)[0]
        cluster_tfidf_sum = np.array(X_tfidf[cluster_indices].sum(axis=0)).flatten()
        top_word_indices = cluster_tfidf_sum.argsort()[::-1][:top_n]
        top_words = [(feature_names[i], cluster_tfidf_sum[i]) for i in top_word_indices]
        cluster_top_words[cluster_id] = top_words
    return cluster_top_words